{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Brazilian Names with N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ibge import load_ibge_name_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130356"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_ibge_name_data()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'maria', 'freq': 11734129},\n",
       " {'name': 'jose', 'freq': 5754529},\n",
       " {'name': 'ana', 'freq': 3089858},\n",
       " {'name': 'joao', 'freq': 2984119},\n",
       " {'name': 'antonio', 'freq': 2576348}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.062996716683544)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(x[\"name\"]) for x in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique characters\n",
    "chars = sorted(set(\"\".join([x[\"name\"] for x in data])))\n",
    "n_chars = len(chars)\n",
    "\n",
    "# Create a mapping from characters to indices and vice versa\n",
    "i_to_c = dict(enumerate([\".\"] + chars))\n",
    "c_to_i = {v: k for k, v in i_to_c.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', '.', '.', 'r'),\n",
       " ('.', '.', 'r', 'a'),\n",
       " ('.', 'r', 'a', 'f'),\n",
       " ('r', 'a', 'f', 'a'),\n",
       " ('a', 'f', 'a', 'e'),\n",
       " ('f', 'a', 'e', 'l'),\n",
       " ('a', 'e', 'l', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ngrams(name: str, n: int) -> Iterable[tuple]:\n",
    "    name = (n - 1)*\".\" + name + \".\"\n",
    "\n",
    "    return zip(*[name[i:] for i in range(n)])\n",
    "\n",
    "list(get_ngrams(\"rafael\", n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_into_ngram_tensors(\n",
    "    data: list[dict[str, Any]], \n",
    "    n: int\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    assert n > 1\n",
    "\n",
    "    input_idxs = []\n",
    "    target_idxs = []\n",
    "    freqs = []\n",
    "\n",
    "    for x in tqdm(data):\n",
    "        name = x[\"name\"]\n",
    "        freq = x[\"freq\"]\n",
    "\n",
    "        for chars in get_ngrams(name, n):\n",
    "            input_idxs.append([c_to_i[c] for c in chars[:-1]])\n",
    "            target_idxs.append(c_to_i[chars[-1]])\n",
    "            freqs.append(freq)\n",
    "\n",
    "    X = F.one_hot(torch.tensor(input_idxs), num_classes=n_chars + 1).float()\n",
    "    X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n",
    "\n",
    "    y = F.one_hot(torch.tensor(target_idxs), num_classes=n_chars + 1).float()\n",
    "\n",
    "    sample_weights = torch.tensor(freqs, dtype=torch.float32)\n",
    "    sample_weights /= sample_weights.sum()\n",
    "\n",
    "    return X, y, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b304306f4fbc4abb8f60fb5b2a19377f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1051060, 243]), torch.Size([1051060, 27]), torch.Size([1051060]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "X, y, sample_weights = encode_data_into_ngram_tensors(data, n)\n",
    "X = X.cuda()\n",
    "y = y.cuda()\n",
    "sample_weights = sample_weights.cuda()\n",
    "\n",
    "X.shape, y.shape, sample_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = torch.randn((n_chars + 1)*(n-1), (n_chars + 1), requires_grad=True, device=\"cuda\")\n",
    "\n",
    "# for i in range(2000 + 1):\n",
    "#     logits = X @ W\n",
    "#     nll = F.cross_entropy(logits, y, reduction=\"none\")\n",
    "#     loss = (nll * sample_weights).sum()\n",
    "\n",
    "#     if i % 100 == 0:\n",
    "#         print(f\"epoch={i} | loss={loss.item():.4f}\")\n",
    "\n",
    "#     W.grad = None\n",
    "#     loss.backward()\n",
    "\n",
    "#     W.data += -1*W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars = []\n",
    "# input_idxs = [0]*(n - 1)\n",
    "\n",
    "# while True:\n",
    "#     x_enc = F.one_hot(torch.tensor(input_idxs), num_classes=n_chars + 1).float().cuda()\n",
    "#     x_enc = x_enc.reshape(1, x_enc.shape[0]*x_enc.shape[1])\n",
    "\n",
    "#     logits = x_enc @ W\n",
    "#     probs = F.softmax(logits, dim=1)\n",
    "#     i = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "#     if i == 0:\n",
    "#         break\n",
    "#     else:\n",
    "#         chars.append(i_to_c[i])\n",
    "#         input_idxs = input_idxs[1:] + [i]\n",
    "\n",
    "# name = ''.join(chars)\n",
    "# name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1105920\n",
      "\n",
      "epoch=0 | loss=263.6970\n",
      "epoch=100 | loss=15.6464\n",
      "epoch=200 | loss=11.0752\n",
      "epoch=300 | loss=8.9190\n",
      "epoch=400 | loss=7.9287\n",
      "epoch=500 | loss=7.7769\n",
      "epoch=600 | loss=6.8230\n",
      "epoch=700 | loss=6.6712\n",
      "epoch=800 | loss=6.3653\n",
      "epoch=900 | loss=5.8965\n",
      "epoch=1000 | loss=5.5615\n",
      "epoch=1100 | loss=5.3102\n",
      "epoch=1200 | loss=5.2796\n",
      "epoch=1300 | loss=4.9411\n",
      "epoch=1400 | loss=4.9241\n",
      "epoch=1500 | loss=4.6746\n",
      "epoch=1600 | loss=4.5524\n",
      "epoch=1700 | loss=4.5694\n",
      "epoch=1800 | loss=4.4374\n",
      "epoch=1900 | loss=4.3373\n",
      "epoch=2000 | loss=4.2501\n",
      "epoch=2100 | loss=3.9660\n",
      "epoch=2200 | loss=3.9098\n",
      "epoch=2300 | loss=3.7644\n",
      "epoch=2400 | loss=3.8642\n",
      "epoch=2500 | loss=3.8481\n",
      "epoch=2600 | loss=3.6856\n",
      "epoch=2700 | loss=3.9951\n",
      "epoch=2800 | loss=3.6995\n",
      "epoch=2900 | loss=3.6568\n",
      "epoch=3000 | loss=3.7076\n",
      "epoch=3100 | loss=3.5633\n",
      "epoch=3200 | loss=3.4684\n",
      "epoch=3300 | loss=3.5558\n",
      "epoch=3400 | loss=3.3842\n",
      "epoch=3500 | loss=3.2934\n",
      "epoch=3600 | loss=3.2694\n",
      "epoch=3700 | loss=3.2832\n",
      "epoch=3800 | loss=3.3880\n",
      "epoch=3900 | loss=3.3134\n",
      "epoch=4000 | loss=3.1620\n",
      "epoch=4100 | loss=3.2607\n",
      "epoch=4200 | loss=3.2134\n",
      "epoch=4300 | loss=3.2147\n",
      "epoch=4400 | loss=3.0297\n",
      "epoch=4500 | loss=3.0932\n",
      "epoch=4600 | loss=3.0528\n",
      "epoch=4700 | loss=3.0741\n",
      "epoch=4800 | loss=3.0161\n",
      "epoch=4900 | loss=3.0655\n",
      "epoch=5000 | loss=3.1041\n",
      "epoch=5100 | loss=2.9284\n",
      "epoch=5200 | loss=2.8771\n",
      "epoch=5300 | loss=2.8746\n",
      "epoch=5400 | loss=2.9537\n",
      "epoch=5500 | loss=2.8252\n",
      "epoch=5600 | loss=2.9977\n",
      "epoch=5700 | loss=2.8875\n",
      "epoch=5800 | loss=2.8894\n",
      "epoch=5900 | loss=2.8653\n",
      "epoch=6000 | loss=2.7569\n",
      "epoch=6100 | loss=2.7686\n",
      "epoch=6200 | loss=2.8159\n",
      "epoch=6300 | loss=2.7153\n",
      "epoch=6400 | loss=2.7085\n",
      "epoch=6500 | loss=2.7814\n",
      "epoch=6600 | loss=2.6903\n",
      "epoch=6700 | loss=2.5991\n",
      "epoch=6800 | loss=2.6572\n",
      "epoch=6900 | loss=2.8625\n",
      "epoch=7000 | loss=2.6614\n",
      "epoch=7100 | loss=2.6397\n",
      "epoch=7200 | loss=2.6518\n",
      "epoch=7300 | loss=2.6905\n",
      "epoch=7400 | loss=2.5368\n",
      "epoch=7500 | loss=2.5730\n",
      "epoch=7600 | loss=2.7447\n",
      "epoch=7700 | loss=2.6350\n",
      "epoch=7800 | loss=2.5951\n",
      "epoch=7900 | loss=2.4994\n",
      "epoch=8000 | loss=2.5541\n",
      "epoch=8100 | loss=2.5067\n",
      "epoch=8200 | loss=2.6839\n",
      "epoch=8300 | loss=2.4932\n",
      "epoch=8400 | loss=2.5591\n",
      "epoch=8500 | loss=2.5012\n",
      "epoch=8600 | loss=2.5152\n",
      "epoch=8700 | loss=2.4345\n",
      "epoch=8800 | loss=2.4903\n",
      "epoch=8900 | loss=2.6127\n",
      "epoch=9000 | loss=2.5550\n",
      "epoch=9100 | loss=2.5454\n",
      "epoch=9200 | loss=2.5077\n",
      "epoch=9300 | loss=2.4957\n",
      "epoch=9400 | loss=2.5418\n",
      "epoch=9500 | loss=2.5105\n",
      "epoch=9600 | loss=2.4517\n",
      "epoch=9700 | loss=2.4563\n",
      "epoch=9800 | loss=2.4283\n",
      "epoch=9900 | loss=2.4383\n",
      "epoch=10000 | loss=2.3926\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20_000\n",
    "n_epochs = 10_000\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_size = int(X.shape[0] * train_ratio)\n",
    "\n",
    "idxs = torch.randperm(X.shape[0], device=\"cuda\")\n",
    "train_idxs = idxs[:train_size]\n",
    "val_idxs = idxs[train_size:]\n",
    "\n",
    "X_train = X[train_idxs, :]\n",
    "y_train = y[train_idxs, :]\n",
    "sw_train = sample_weights[train_idxs]\n",
    "sw_train /= sw_train.sum()\n",
    "\n",
    "X_val = X[val_idxs, :]\n",
    "y_val = y[val_idxs, :]\n",
    "sw_val = sample_weights[val_idxs]\n",
    "sw_val /= sw_val.sum()\n",
    "\n",
    "W1 = torch.randn((n_chars + 1)*(n-1), 2**12, requires_grad=True, device=\"cuda\")\n",
    "W2 = torch.randn(2**12, (n_chars + 1), requires_grad=True, device=\"cuda\")\n",
    "\n",
    "def forward(X):\n",
    "    H1 = F.relu(X @ W1)\n",
    "    logits = H1 @ W2\n",
    "    return logits\n",
    "\n",
    "def compute_weighted_nll(logits, y, sample_weights):\n",
    "    nll = F.cross_entropy(logits, y, reduction=\"none\")\n",
    "    return (nll * sample_weights).sum()\n",
    "\n",
    "print(f\"Model size: {W1.numel() + W2.numel()}\\n\")\n",
    "\n",
    "for i in range(n_epochs + 1):\n",
    "    sgd_idxs = torch.randint(0, X_train.shape[0], (batch_size,), device=\"cuda\")\n",
    "\n",
    "    X_sgd = X_train[sgd_idxs, :]\n",
    "    y_sgd = y_train[sgd_idxs, :]\n",
    "    sw_sgd = sw_train[sgd_idxs]\n",
    "    sw_sgd /= sw_sgd.sum()\n",
    "\n",
    "    logits = forward(X_sgd)\n",
    "    loss = compute_weighted_nll(logits, y_sgd, sw_sgd)\n",
    "\n",
    "    if i % (n_epochs // 100) == 0:\n",
    "        loss_val = compute_weighted_nll(forward(X_val), y_val, sw_val)\n",
    "        print(f\"epoch={i} | loss={loss_val.item():.4f}\")\n",
    "\n",
    "    W1.grad = None\n",
    "    W2.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data += -1*W1.grad\n",
    "    W2.data += -1*W2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edvaldo'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(\"edv\")\n",
    "padding_size = n - 1 - len(chars)\n",
    "input_idxs = [0]*padding_size + [c_to_i[c] for c in chars]\n",
    "\n",
    "while True:\n",
    "    x_enc = F.one_hot(torch.tensor(input_idxs), num_classes=n_chars + 1).float().cuda()\n",
    "    x_enc = x_enc.reshape(1, x_enc.shape[0]*x_enc.shape[1])\n",
    "\n",
    "    h1 = F.relu(x_enc @ W1)\n",
    "    logits = h1 @ W2\n",
    "\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    i = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    if i == 0:\n",
    "        break\n",
    "    else:\n",
    "        chars.append(i_to_c[i])\n",
    "        input_idxs = input_idxs[1:] + [i]\n",
    "\n",
    "name = ''.join(chars)\n",
    "name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
